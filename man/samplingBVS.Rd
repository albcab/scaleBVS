% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/samplingBVS.r
\name{samplingBVS}
\alias{samplingBVS}
\title{samples from the posterior distribution of a Bayesian Variable Selection model using weighted Tempered Gibbs Sampling}
\usage{
samplingBVS(y, X, c = NULL, h = c(1, 1), n_iter = 2000,
  burn_in = 0.2, k_weight = 0, weighted = TRUE)
}
\arguments{
\item{y}{a vector of n observations (dependent variable) with dimensions (nx1).}

\item{X}{a matrix of p regressors (independent variables) with dimension (nxp).}

\item{c}{a real number greater than 0 which serves as a constant of proportionality
to the specification of the prior covariance matrix of the coefficients of the
selected regressors in the linear regression. The default is \code{NULL} which yields the recommended constant of proportionality for Zellner's g-prior 
, i.e. c = n.}

\item{h}{either a real number greater than 0 and smaller than 1 or a vector of real
values, both greater than 0. This parameter specifies the prior information of the
inclusion probability of the regressors which is identical for all regressors. 
In the former case, the prior probability is set to a fixed value. In the latter 
case, the prior probability is a Beta distribution with the specified parameters.
The default is the uniform distribution in terms of a Beta prior \code{c(1,1)}.}

\item{n_iter}{a positive integer specifying the number of iterations for the Markov
Chain. The default is 2000.}

\item{burn_in}{either an integer greater than 1 or a real number greater than 0 and
smaller than 1. Specifies the number of burn in iterations for the Markov Chain. In
the former case the burn in iterations are set the fixed integer. In the latter case
the number of iterations are the specified percentage of the number of iterations.
The default is 0.2.}

\item{k_weight}{a real number greater than 0 which, in the case of \code{weighted = TRUE},
controls the tradeoff between exploration and exploitation in the choice of the variable
to be flipped at each iteration. A larger \code{k_weight} favours exploration. The default is 0.}

\item{weighted}{logical, with default \code{TRUE}, indicating whether to perform
weighted Tempered Gibbs Sampling if \code{TRUE} or Tempered Gibbs Sampling if 
\code{FALSE}.}
}
\value{
A list with named objects:
\item{PIP }{a vector (px1) containing Rao-Blackwellised estimators of 
the marginal PIPs for each of the p regressors in \code{X}.}
\item{states }{a list containing the elements necessary to reproduce the samples of
the Markov Chain. These elements are:\cr
"start" - starting value for \eqn{\gamma} after the burnin period.\cr
"sample_weights" - a vector (n_iterx1) of weights for \eqn{\gamma} at each step of 
the Markov Chain.\cr
"indices_sequence" - a vector (n_iterx1) of indices ranging \eqn{1,...,p} indicating the
element of \eqn{\gamma} flipped at each step of the Markov Chain.}
}
\description{
Perform Bayesian variable selection in linear regression contexts 
using discrete spike and slab priors. Posterior sampling and calculation 
of marginal Posterior Inclusion Probabilities (PIPs) for explanatory 
variables is done using the weighted Tempered Gibbs Sampling algorithm 
of Zanella and Roberts (2019).
}
\details{
The evaluated linear regression model can be written as
\deqn{Y|\beta_\gamma, \gamma, \sigma^2 ~ N(X_\gamma\beta_\gamma,\sigma^2(I_n))}
\deqn{\beta_\gamma|\gamma, \sigma^2 ~ N(0,\sigma^2\Sigma_\gamma)}
\deqn{p(\sigma^2) \propto 1/\sigma^2}
\deqn{\gamma_i|h iid~ Bern(h)       i = 1,...,p}
where the posterior probability of interest is \eqn{p(\gamma|Y)}.

The prior covariance matrix of the coefficients 
of the selected regressors is \eqn{\Sigma_\gamma = c(X_\gamma^TX_\gamma)}, i.e. the g-prior recommended by Zellner 
(1986).

The Rao-Blackwellized estimators provide a vector with inclusion probabilities for 
each of the regressors, \eqn{{p(\gamma_i=1|Y)}_{i=1}^{p}}.

\eqn{h} can be a fixed value or \eqn{h ~ Beta(a,b)}.

The sampling algorithm flips one of p binary values of \eqn{\gamma} by sampling \eqn{i} 
from \eqn{1,...,p} proportionally to \eqn{p_i(\gamma)=p(\gamma_i|\gamma_{-i},Y)^{-1}}
in the case of Tempered Gibbs Sampling and proportionally to 
\eqn{p_i(\gamma)=(p(\gamma_i=1|\gamma_{-i},Y)+k/p)/p(\gamma_i|\gamma_{-i},Y)}
in the case of weighted Tempered Gibbs Sampling. Also, the weight of the new state of
the Markov Chain is proportional to \eqn{(\sum_{i=1}^p p_i(\gamma))^{-1}}.

For more information on weighted Tempered Gibbs Sampling, please refer to Zanella and
Roberts (2019).
}
\examples{
#Posterior inclusion probabilities of characteristics of cars on describing mileage

#load data
data(mtcars)

#create X matrix and y vector with zero mean for all regressors
X <- t(t(mtcars[,-1]) - colMeans(mtcars[,-1]))
y <- mtcars$mpg - mean(mtcars$mpg)

mtcars.output <- samplingBVS(y, X)

names(mtcars.output$PIP) <- names(mtcars[,-1])
print(mtcars.output$PIP)
}
\references{
Zanella, G. and Roberts, G. (2019). Scalable importance tempering and Bayesian variable selection. Journal of the Royal Statistical Society: Series B (Statistical Methodology): 489â€“517. Crossref. Web.

Zellner, A. (1986). On Assessing Prior Distributions and Bayesian Regression Analysis with g-Prior Distributions. In: Goel, P. and Zellner, A., Eds., Bayesian Inference and Decision Techniques: Essays in Honor of Bruno de Finetti, Elsevier Science Publishers, Inc., New York, 233-243.
}
\seealso{
\code{\link{createSamples}} for creating the samples of the Markov Chain and their weights used to calculate the PIPs.
}
